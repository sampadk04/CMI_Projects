{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "- Sampad Kumar Kar\n",
    "- MCS202215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle files\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# import regular expressions\n",
    "import re\n",
    "\n",
    "# pre-processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLKT preprocessing imports\n",
    "from nltk import download\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to view loop progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# to generate random no.s\n",
    "import random\n",
    "\n",
    "# to save dictionary\n",
    "import pickle\n",
    "\n",
    "# import priority queue\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sampadk04/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sampadk04/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sampadk04/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Corpus Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename):\n",
    "    '''\n",
    "    Input: file path\n",
    "    Return: title, abstract, content\n",
    "\n",
    "    Description: extracts the title, abstract and body from the paper\n",
    "    '''\n",
    "    file = open(filename)\n",
    "    content = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "    # init empty strings to store data\n",
    "    title, abstract, body_text = \"\", \"\", \"\"\n",
    "\n",
    "    if 'title' in content['metadata']:\n",
    "        title = content['metadata']['title']\n",
    "    \n",
    "    if 'abstract' in content:\n",
    "        for abstract_data in content['abstract']:\n",
    "            abstract += abstract_data['text']\n",
    "    \n",
    "    if 'body_text' in content:\n",
    "        for text_snippet in content['body_text']:\n",
    "            if 'text' in text_snippet:\n",
    "                body_text += text_snippet['text']\n",
    "\n",
    "    return title.lower(), abstract.lower(), body_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/pdf_json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = os.path.join(os.pardir, 'data', 'pdf_json')\n",
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of .json files: 56528\n"
     ]
    }
   ],
   "source": [
    "json_file_paths = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "print(\"Total # of .json files:\", len(json_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: an extended outbreak of infectious peritonitis in a closed colony of european wildcats (fells silvestris)\n",
      "\n",
      "Abstract: feline infectious peritonitis is a multisystemic disease of domestic and exotic cats caused by a coronavirus. an outbreak of feline infectious peritonitis was investigated in a closed colony of european wildcats (fells silvestris) at a zoological garden. over a six-year period, a putative fading kitten syndrome occurred in six of 11 litters born and severe lesions of infectious peritonitis occurred in five of the eight wildcats retained in the colony during this period. lesions were more acute in the early stages of the outbreak and included perivascular pyogranulomatons inflammation with exudative serositis. lesions occurred only in males. vascular lesions were common in the liver of all affected wildcats, serositis occurred in the abdominal and thoracic cavities in most cases and meningeal lesions were present in two cases. immunohistochemistry with specific antisera detected viral antigen within macrophages in all lesions. this outbreak demonstrates that the lesions of feline infectious peritonitis can become modified over time and that the virus can persist in a closed colony, possibly via carrier wildcats.\n",
      "\n",
      "Content: feline infectious peritonitis virus (fipv) is a member of the family coronaviridae and is a single stranded rna virus with three major structural proteins, including a nucleocapsid protein (lai, 1990) . studies on the pathogenesis of fipv in both domestic and exotic cats have demonstrated that an acute, fulminant disease (so-called \"wet\" or \"effusive\" fip) can occur on challenge of seropositive cats as compared with a milder response (so-called \"dry\" or \"non-effusive\" fip) in seronegative cats (for a review see pedersen, 1983) . it is believed that the presence of pre-existing antibody leads to either immune complex deposition (jacobese-geels, daha and horzinek, 1982) or antibodydependent enhancement of macrophage infection upon challenge by fipv (olsen, corapi, ngichabe, baines and scott, 1992) . therefore, naturally and experimentally ini~cted domestic cats develop lesions with a marked variation in intensity, depending on the cat's antibody status.the lesions in domestic cats infect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the 'extract_data function'\n",
    "# ignore this cell\n",
    "sample_file_path = os.path.join(os.pardir, 'data', 'sample_file.json')\n",
    "\n",
    "title, abstract, content = extract_data(sample_file_path)\n",
    "print(\"Title:\", title)\n",
    "print(\"\")\n",
    "print(\"Abstract:\", abstract)\n",
    "print(\"\")\n",
    "print(\"Content:\", content[:1000])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define various functions for pre-processing\n",
    "\n",
    "def remove_non_alpha(input_text):\n",
    "    '''\n",
    "    Removes non alphabets (including numbers, punctuations, special characters etc.)\n",
    "    '''\n",
    "    tokenizer = RegexpTokenizer(\"[A-Za-z_]+\")\n",
    "    new_text = tokenizer.tokenize(input_text)\n",
    "    new_text = \" \".join(new_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_non_alphanumeric_pt(input_text):\n",
    "    '''\n",
    "    Removes non alphabets and non numerics and non (. ! ?) entities (special characters and other punctuations etc.)\n",
    "    '''\n",
    "    # Define a regular expression to match words, numbers, and full-stops\n",
    "    pattern = re.compile(r'[a-zA-Z0-9_.?!]+')\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    new_text = tokenizer.tokenize(input_text)\n",
    "    new_text = \" \".join(new_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def remove_html(input_text):\n",
    "    '''\n",
    "    Removes html links from the document\n",
    "    '''\n",
    "    # define regular exp to match html urls and replace them with \"\"\n",
    "    new_text = re.sub(r'(http[s]?://|www\\.)[^\\s]+', \"\", input_text)\n",
    "    return new_text\n",
    "\n",
    "def word_lemmatizer(input_text):\n",
    "    '''\n",
    "    This performs word lemmatization, converts words to meaningful base form\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_text = [lemmatizer.lemmatize(word) for word in input_text.split()]\n",
    "    new_text = \" \".join(new_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    '''\n",
    "    This removes stopwords from the text, useful for problems like sentiment analysis\n",
    "    \n",
    "    Note: the input_text should contain only lowercase words\n",
    "    '''\n",
    "    # form a set stopwords\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    new_text = [w for w in input_text.split() if w not in stopwords_set]\n",
    "    new_text = \" \".join(new_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the functions to create a relevant corpus\n",
    "\n",
    "def text_cleaner_1(input_text):\n",
    "    '''\n",
    "    Use this to deep clean the texts for problems like sentiment analysis\n",
    "    '''\n",
    "    new_text = remove_html(input_text)\n",
    "    new_text = remove_non_alpha(new_text)\n",
    "    new_text = word_lemmatizer(new_text)\n",
    "\n",
    "    return new_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the functions to create a relevant corpus\n",
    "\n",
    "def text_cleaner_2(input_text, sentence_breaker=False):\n",
    "    '''\n",
    "    Use this to shallow clean the texts for n-gram models and sentence completion\n",
    "\n",
    "    sentence_breaker: If this is true, this adds sentence breaker ' # ' before every sentence; (useful while building language models)\n",
    "    '''\n",
    "    new_text = remove_html(input_text)\n",
    "    new_text = remove_non_alphanumeric_pt(new_text)\n",
    "    new_text = new_text.strip()\n",
    "\n",
    "    if sentence_breaker:\n",
    "        sentence_list = []\n",
    "        sentences = re.split(\"[.!?]\", new_text)\n",
    "        for sentence in sentences:\n",
    "            sentence_list.append(\" # \" + sentence)\n",
    "        # join the list of sentences into a single text\n",
    "        new_text = ''.join(sentence_list)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the texts into a .txt file\n",
    "\n",
    "def save_corpus(json_file_paths, txt_save_path, type):\n",
    "    '''\n",
    "    Saves text into .txt file\n",
    "\n",
    "    type=1: uses text_cleaner_1, deep clean\n",
    "    type=2: uses text_cleaner_2, shallow clean\n",
    "    type=3: uses text_cleaner_3, shallow cleaning + sentence separation\n",
    "    '''\n",
    "    corpus = open(txt_save_path, 'w')\n",
    "\n",
    "    for json_file_path in tqdm(json_file_paths):\n",
    "        # extract title, abstract, body\n",
    "        title, abstract, body = extract_data(json_file_path)\n",
    "        \n",
    "        # update after cleaning\n",
    "        if type==1:\n",
    "            # use text_cleaner_1 to deep clean everything\n",
    "            title, abstract, body = text_cleaner_1(title), text_cleaner_1(abstract), text_cleaner_1(body)\n",
    "\n",
    "            # combine the texts\n",
    "            total_content = title + abstract + body + '\\n'\n",
    "            \n",
    "            # write into the .txt file\n",
    "            corpus.write(total_content)\n",
    "\n",
    "        elif type==2:\n",
    "            # use text_cleaner_1 for titles, to deep clean\n",
    "            title = text_cleaner_1(title) + '. '\n",
    "            # use text_cleaner_2 for abstract, body to shallow clean\n",
    "            abstract = text_cleaner_2(abstract)\n",
    "            body = text_cleaner_2(body)\n",
    "\n",
    "            # combine the texts\n",
    "            total_content = title + abstract + body + '\\n'\n",
    "\n",
    "            # write into the .txt file\n",
    "            corpus.write(total_content)        \n",
    "        elif type==3:\n",
    "            # use text_cleaner_1 for titles, to deep clean\n",
    "            title = ' # ' + text_cleaner_1(title)\n",
    "            # use text_cleaner_3 for abstract, body to shallow clean\n",
    "            abstract = text_cleaner_2(abstract, sentence_breaker=True)\n",
    "            body = text_cleaner_2(body, sentence_breaker=True)\n",
    "\n",
    "            # combine the texts\n",
    "            total_content = title + abstract + body + '\\n'\n",
    "\n",
    "            # write into the .txt file\n",
    "            corpus.write(total_content)\n",
    "        else:\n",
    "            print(\"Choose either type== 1/2/3\")\n",
    "\n",
    "    corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/my_corpus'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus_folder_path = os.path.join(os.pardir, 'data', 'my_corpus')\n",
    "my_corpus_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the corpus text files\n",
    "corpus_1_path = os.path.join(my_corpus_folder_path, 'corpus_1.txt')\n",
    "corpus_2_path = os.path.join(my_corpus_folder_path, 'corpus_2.txt')\n",
    "corpus_3_path = os.path.join(my_corpus_folder_path, 'corpus_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56528/56528 [12:42<00:00, 74.13it/s] \n"
     ]
    }
   ],
   "source": [
    "save_corpus(json_file_paths, corpus_1_path, type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56528/56528 [01:52<00:00, 503.40it/s]\n"
     ]
    }
   ],
   "source": [
    "save_corpus(json_file_paths, corpus_2_path, type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56528/56528 [02:02<00:00, 460.50it/s]\n"
     ]
    }
   ],
   "source": [
    "save_corpus(json_file_paths, corpus_3_path, type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finding Vocabulary Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_freq(corpus_path):\n",
    "    corpus_vocab = set()\n",
    "    corpus_vocab_freq = dict()\n",
    "    paper_count = 0\n",
    "\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|\\S+')\n",
    "\n",
    "    corpus = open(corpus_path, 'r')\n",
    "    \n",
    "    # read the papers one after another and process\n",
    "    # Note: we consider sentence[:-1] to remove the '\\n' character\n",
    "    paper_data = corpus.readline()[:-1]\n",
    "\n",
    "    # run the loop till we reach the end of the document\n",
    "    while paper_data:\n",
    "        # tokenize the paper\n",
    "        paper_words = tokenizer.tokenize(paper_data)\n",
    "        # process word by word\n",
    "        for word in paper_words:\n",
    "            if word not in corpus_vocab:\n",
    "                corpus_vocab.add(word)\n",
    "                corpus_vocab_freq[word] = 1\n",
    "            else:\n",
    "                corpus_vocab_freq[word] += 1\n",
    "        \n",
    "        # read the next paper_data\n",
    "        paper_data = corpus.readline()[:-1]\n",
    "\n",
    "        # increase paper_count\n",
    "        paper_count += 1\n",
    "        # print info to keep track of progress\n",
    "        if paper_count%10000 == 0:\n",
    "            print(\"-\"*20)\n",
    "            print(f\"Document #: {paper_count}; Vocabulary Size: {len(corpus_vocab)}\")\n",
    "\n",
    "    corpus.close()\n",
    "\n",
    "    return corpus_vocab, corpus_vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Document #: 10000; Vocabulary Size: 429377\n",
      "--------------------\n",
      "Document #: 20000; Vocabulary Size: 660534\n",
      "--------------------\n",
      "Document #: 30000; Vocabulary Size: 846587\n",
      "--------------------\n",
      "Document #: 40000; Vocabulary Size: 1024412\n",
      "--------------------\n",
      "Document #: 50000; Vocabulary Size: 1170000\n",
      "Corpus-2 Vocabulary Size: 1260153\n"
     ]
    }
   ],
   "source": [
    "corpus_2_vocab, corpus_2_freq = find_words_freq(corpus_2_path)\n",
    "\n",
    "print(\"Corpus-2 Vocabulary Size:\", len(corpus_2_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bigram and Trigram Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 `n-gram` Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_ngrams(corpus_path, n=2, smoothing_factor=1):\n",
    "    ngram_token_freq = Counter()\n",
    "    corpus_vocab = set()\n",
    "    corpus_vocab_size = 0\n",
    "    paper_count = 0\n",
    "    # to be used as denominator of laplacian smoothing\n",
    "    token_count = 0\n",
    "    \n",
    "    corpus = open(corpus_path, 'r')\n",
    "    # read the papers one after another and process\n",
    "    # Note: we consider sentence[:-1] to remove the '\\n' character\n",
    "    paper_data = corpus.readline()[:-1]\n",
    "    \n",
    "    # run the loop till we reach the end of the document\n",
    "    while paper_data:\n",
    "        # tokenize the paper\n",
    "        paper_words = word_tokenize(paper_data)\n",
    "        \n",
    "        # extract this as n-grams\n",
    "        for i in range(len(paper_words)-n+1):\n",
    "            ngram = tuple(paper_words[i:i+n])\n",
    "            # update the ngram token freq\n",
    "            ngram_token_freq[ngram] += 1\n",
    "        \n",
    "        # update the vocab\n",
    "        for word in paper_words:\n",
    "            if word not in corpus_vocab:\n",
    "                corpus_vocab.add(word)\n",
    "                corpus_vocab_size += 1\n",
    "        \n",
    "        # read the next paper_data\n",
    "        paper_data = corpus.readline()[:-1]\n",
    "\n",
    "        # increase paper_count\n",
    "        paper_count += 1\n",
    "\n",
    "        # print info to keep track of progress\n",
    "        if paper_count%10000 == 0:\n",
    "            print(\"-\"*20)\n",
    "            print(f\"Document #: {paper_count}; Vocabulary Size: {len(corpus_vocab)}\")\n",
    "    \n",
    "    corpus.close()\n",
    "\n",
    "    # conduct laplacian smoothing\n",
    "    for ngram in ngram_token_freq:\n",
    "        ngram_token_freq[ngram] += smoothing_factor\n",
    "        # update the token_count for smoothing denominator\n",
    "        token_count += ngram_token_freq[ngram]\n",
    "\n",
    "    # prob\n",
    "    ngram_token_prob = ngram_token_freq.copy()\n",
    "    \n",
    "    # log(a/b) = log(a) - log(b)\n",
    "    for ngram in ngram_token_freq:\n",
    "        ngram_token_prob[ngram] = ngram_token_freq[ngram]/token_count\n",
    "\n",
    "    return ngram_token_prob, corpus_vocab, corpus_vocab_size, token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Document #: 10000; Vocabulary Size: 400690\n",
      "--------------------\n",
      "Document #: 20000; Vocabulary Size: 615733\n",
      "--------------------\n",
      "Document #: 30000; Vocabulary Size: 787940\n",
      "--------------------\n",
      "Document #: 40000; Vocabulary Size: 952844\n",
      "--------------------\n",
      "Document #: 50000; Vocabulary Size: 1087097\n"
     ]
    }
   ],
   "source": [
    "# corpus path\n",
    "corpus_3_path = os.path.join(my_corpus_folder_path, 'corpus_3.txt')\n",
    "\n",
    "# Generate the bi-gram language model\n",
    "bigram_model, bigram_vocab, bigram_vocab_size, bigram_token_count = generate_ngrams(corpus_path=corpus_3_path, n=2, smoothing_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_path = os.path.join('ngram_models', 'bigram_model.pickle')\n",
    "bigram_vocab_path = os.path.join('ngram_models', 'bigram_vocab.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the models\n",
    "\n",
    "fh = open(bigram_model_path, \"wb\")\n",
    "pickle.dump(bigram_model, fh)\n",
    "fh.close()\n",
    "\n",
    "\n",
    "fh = open(bigram_vocab_path, \"wb\")\n",
    "pickle.dump(bigram_vocab, fh)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the models\n",
    "\n",
    "fh = open(bigram_model_path, 'rb')\n",
    "bigram_model = pickle.load(fh)\n",
    "fh.close()\n",
    "\n",
    "fh = open(bigram_vocab_path, 'rb')\n",
    "bigram_vocab = pickle.load(fh)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Vocabulary Size: 1170327\n",
      "Bigram Token Count: 270756039\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram Vocabulary Size:\", bigram_vocab_size)\n",
    "print(\"Bigram Token Count:\", bigram_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Trigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Document #: 10000; Vocabulary Size: 400690\n",
      "--------------------\n",
      "Document #: 20000; Vocabulary Size: 615733\n",
      "--------------------\n",
      "Document #: 30000; Vocabulary Size: 787940\n",
      "--------------------\n",
      "Document #: 40000; Vocabulary Size: 952844\n",
      "--------------------\n",
      "Document #: 50000; Vocabulary Size: 1087097\n"
     ]
    }
   ],
   "source": [
    "# corpus path\n",
    "corpus_3_path = os.path.join(my_corpus_folder_path, 'corpus_3.txt')\n",
    "\n",
    "# Generate the bi-gram language model\n",
    "trigram_model, trigram_vocab, trigram_vocab_size, trigram_token_count = generate_ngrams(corpus_path=corpus_3_path, n=3, smoothing_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_path = os.path.join('ngram_models', 'trigram_model.pickle')\n",
    "trigram_vocab_path = os.path.join('ngram_models', 'trigram_vocab.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the models\n",
    "\n",
    "fh = open(trigram_model_path, \"wb\")\n",
    "pickle.dump(trigram_model, fh)\n",
    "fh.close()\n",
    "\n",
    "fh = open(trigram_vocab_path, \"wb\")\n",
    "pickle.dump(trigram_vocab, fh)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the models\n",
    "\n",
    "fh = open(trigram_model_path, 'rb')\n",
    "trigram_model = pickle.load(fh)\n",
    "fh.close()\n",
    "\n",
    "fh = open(trigram_vocab_path, 'rb')\n",
    "trigram_vocab = pickle.load(fh)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Vocabulary Size: 1170327\n",
      "Trigram Token Count: 335690076\n"
     ]
    }
   ],
   "source": [
    "print(\"Trigram Vocabulary Size:\", trigram_vocab_size)\n",
    "print(\"Trigram Token Count:\", trigram_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predicting Missing Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next(ngram_model, ngram_vocab, word_tuple):\n",
    "    best_prob = -np.inf\n",
    "    best_word = \"\"\n",
    "    for word in ngram_vocab:\n",
    "        ngram = word_tuple + (word,)\n",
    "        if ngram in ngram_model:\n",
    "            word_found = True\n",
    "            curr_prob = ngram_model[ngram]\n",
    "\n",
    "            # update best word\n",
    "            if best_prob < curr_prob:\n",
    "                best_prob = curr_prob\n",
    "                best_word = word\n",
    "    \n",
    "    # if no ngram could be formed\n",
    "    while not best_word:\n",
    "        # output random word\n",
    "        best_word = random.sample(ngram_vocab, 1)[0]\n",
    "        if not best_word.isalpha():\n",
    "            best_word=\"\"\n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def fill_blank(sentence_before, sentence_after, n, smoothing_factor, ngram_model, ngram_vocab, ngram_token_count, keep_top_k=10):\n",
    "    # convert the sentences to tuples\n",
    "    sentence_tuple_before = tuple(word_tokenize(text_cleaner_2(sentence_before)))\n",
    "    sentence_tuple_after = tuple(word_tokenize(text_cleaner_2(sentence_after)))\n",
    "    \n",
    "    # add the sentence breaker\n",
    "    sentence_tuple_before = ('#',) + sentence_tuple_before\n",
    "    sentence_tuple_after = sentence_tuple_after + ('#',)\n",
    "    \n",
    "    # resize sentence tuples before/after\n",
    "    sentence_tuple_before = sentence_tuple_before[-(n-1):]\n",
    "    sentence_tuple_after = sentence_tuple_after[:(n-1)]\n",
    "    \n",
    "    # default smoothing prob for non-existent ngram tuples\n",
    "    # default_prob = smoothing_factor/ngram_token_count\n",
    "    default_logprob = np.log(smoothing_factor) - np.log(ngram_token_count)\n",
    "    \n",
    "    best_logprob_sum = n*default_logprob\n",
    "    best_word = \"\"\n",
    "\n",
    "    k = keep_top_k\n",
    "    top_k_queue = []\n",
    "    \n",
    "    for word in ngram_vocab:\n",
    "        sentence_tuple = sentence_tuple_before + (word,) + sentence_tuple_after\n",
    "\n",
    "        curr_logprob_sum = 0\n",
    "        for i in range(n):\n",
    "            curr_logprob = default_logprob\n",
    "            curr_tuple = sentence_tuple[i:i+n]\n",
    "            if curr_tuple in ngram_model:\n",
    "                curr_logprob = np.log(ngram_model[curr_tuple])\n",
    "            curr_logprob_sum += curr_logprob\n",
    "        \n",
    "        # update top-k for alphabets\n",
    "        if word.isalpha() or word=='#':\n",
    "            if len(top_k_queue) < k:\n",
    "                heapq.heappush(top_k_queue, (curr_logprob_sum, word))\n",
    "            else:\n",
    "                smallest_logprob_sum = top_k_queue[0][0]\n",
    "                if smallest_logprob_sum < curr_logprob_sum:\n",
    "                    heapq.heappop(top_k_queue)\n",
    "                    heapq.heappush(top_k_queue, (curr_logprob_sum, word))\n",
    "\n",
    "\n",
    "        # update current\n",
    "        if best_logprob_sum < curr_logprob_sum:\n",
    "            best_logprob_sum = curr_logprob_sum\n",
    "            best_word = word\n",
    "    \n",
    "    # if no ngram could be formed\n",
    "    while not best_word:\n",
    "        # output random word\n",
    "        best_word = random.sample(ngram_vocab, 1)[0]\n",
    "        if not best_word.isalpha() or best_word!='#':\n",
    "            best_word=\"\"\n",
    "    \n",
    "\n",
    "    top_k_queue = sorted(top_k_queue, reverse=True)\n",
    "    print(\"Top \" + str(k) + \" words:\")\n",
    "    for (logprob_sum, word) in top_k_queue:\n",
    "        print(word + \"               || LogProbSum: \" + str(logprob_sum))\n",
    "    \n",
    "    print(\"\\nBest replacement:\", best_word)\n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Prediction using `bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "in               || LogProbSum: -24.178234921005483\n",
      "the               || LogProbSum: -25.0418257824373\n",
      "not               || LogProbSum: -25.17283388945549\n",
      "mechanically               || LogProbSum: -25.772118821044984\n",
      "a               || LogProbSum: -26.423424119183927\n",
      "of               || LogProbSum: -26.50056719523603\n",
      "well               || LogProbSum: -26.78515038997943\n",
      "non               || LogProbSum: -26.9383355504725\n",
      "then               || LogProbSum: -26.99519296801243\n",
      "all               || LogProbSum: -27.118759216463545\n",
      "\n",
      "Best replacement: in\n"
     ]
    }
   ],
   "source": [
    "# all houses were ----- ventilated\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"all houses were\",\n",
    "    sentence_after=\"ventilated\",\n",
    "    n=2,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=bigram_model,\n",
    "    ngram_vocab=bigram_vocab,\n",
    "    ngram_token_count=bigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "and               || LogProbSum: -21.97138030600042\n",
      "#               || LogProbSum: -22.22456839272651\n",
      "approach               || LogProbSum: -23.3935481010631\n",
      "model               || LogProbSum: -24.782088564380153\n",
      "response               || LogProbSum: -24.839986544580505\n",
      "data               || LogProbSum: -25.31097929839438\n",
      "as               || LogProbSum: -25.32580450790242\n",
      "system               || LogProbSum: -25.332981023426925\n",
      "in               || LogProbSum: -25.34902375675157\n",
      "care               || LogProbSum: -25.527458426268524\n",
      "\n",
      "Best replacement: and\n"
     ]
    }
   ],
   "source": [
    "# it aims to develop an integrated ------ to reach mmps exposed to malaria with prevention diagnosis and treatment\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"it aims to develop an integrated\",\n",
    "    sentence_after=\"to reach mmps exposed to malaria with prevention diagnosis and treatment\",\n",
    "    n=2,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=bigram_model,\n",
    "    ngram_vocab=bigram_vocab,\n",
    "    ngram_token_count=bigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "#               || LogProbSum: -18.55200510901936\n",
      "and               || LogProbSum: -20.09305360213994\n",
      "or               || LogProbSum: -22.06827677129898\n",
      "is               || LogProbSum: -22.52638125949056\n",
      "for               || LogProbSum: -22.626105864672997\n",
      "of               || LogProbSum: -22.69998231919469\n",
      "caused               || LogProbSum: -23.3427265291346\n",
      "followed               || LogProbSum: -23.493684634711894\n",
      "induced               || LogProbSum: -23.83545623113725\n",
      "with               || LogProbSum: -24.04668101888194\n",
      "\n",
      "Best replacement: #\n"
     ]
    }
   ],
   "source": [
    "# malaria with prevention diagnosis and treatment ------ by involving non-health\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"malaria with prevention diagnosis and treatment\",\n",
    "    sentence_after=\"by involving non-health\",\n",
    "    n=2,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=bigram_model,\n",
    "    ngram_vocab=bigram_vocab,\n",
    "    ngram_token_count=bigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "and               || LogProbSum: -22.93228801500535\n",
      "of               || LogProbSum: -24.000938853931473\n",
      "#               || LogProbSum: -24.26434220905029\n",
      "care               || LogProbSum: -25.6829149089419\n",
      "the               || LogProbSum: -26.278684116810048\n",
      "system               || LogProbSum: -26.984309826275307\n",
      "with               || LogProbSum: -27.406874541463374\n",
      "to               || LogProbSum: -27.566534877101603\n",
      "for               || LogProbSum: -27.70441668196615\n",
      "related               || LogProbSum: -28.02984924536389\n",
      "\n",
      "Best replacement: and\n"
     ]
    }
   ],
   "source": [
    "# by involving non-health ----- stakeholders from provincial to community level\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"by involving non health\",\n",
    "    sentence_after=\"stakeholders from provincial to community level\",\n",
    "    n=2,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=bigram_model,\n",
    "    ngram_vocab=bigram_vocab,\n",
    "    ngram_token_count=bigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Prediction using `trigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "mechanically               || LogProbSum: -49.05550787152691\n",
      "not               || LogProbSum: -55.49389931655065\n",
      "invasively               || LogProbSum: -55.59925983220849\n",
      "well               || LogProbSum: -55.89936442465881\n",
      "and               || LogProbSum: -55.89936442465882\n",
      "then               || LogProbSum: -56.25603936859755\n",
      "mechanical               || LogProbSum: -56.59251160521876\n",
      "artificially               || LogProbSum: -56.697872120876596\n",
      "still               || LogProbSum: -56.81565515653297\n",
      "curtain               || LogProbSum: -56.81565515653297\n",
      "\n",
      "Best replacement: mechanically\n"
     ]
    }
   ],
   "source": [
    "# all houses were ----- ventilated\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"all houses were\",\n",
    "    sentence_after=\"ventilated\",\n",
    "    n=3,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=trigram_model,\n",
    "    ngram_vocab=trigram_vocab,\n",
    "    ngram_token_count=trigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "approach               || LogProbSum: -46.994788632685186\n",
      "and               || LogProbSum: -49.655585948936846\n",
      "system               || LogProbSum: -50.319445937225\n",
      "model               || LogProbSum: -50.775400445255556\n",
      "way               || LogProbSum: -51.63106655531328\n",
      "time               || LogProbSum: -51.91409095749108\n",
      "strategy               || LogProbSum: -52.40285686319234\n",
      "platform               || LogProbSum: -52.42120600186054\n",
      "response               || LogProbSum: -52.74662840229516\n",
      "sample               || LogProbSum: -52.8266711099687\n",
      "\n",
      "Best replacement: approach\n"
     ]
    }
   ],
   "source": [
    "# it aims to develop an integrated ------ to reach mmps exposed to malaria with prevention diagnosis and treatment\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"it aims to develop an integrated\",\n",
    "    sentence_after=\"to reach mmps exposed to malaria with prevention diagnosis and treatment\",\n",
    "    n=3,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=trigram_model,\n",
    "    ngram_vocab=trigram_vocab,\n",
    "    ngram_token_count=trigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "#               || LogProbSum: -46.40009276083042\n",
      "and               || LogProbSum: -49.37550847587029\n",
      "of               || LogProbSum: -50.49488686228239\n",
      "is               || LogProbSum: -50.94777167119635\n",
      "options               || LogProbSum: -51.38960442347539\n",
      "or               || LogProbSum: -51.90300027079692\n",
      "guidelines               || LogProbSum: -52.05348122173522\n",
      "strategies               || LogProbSum: -52.212988100763\n",
      "groups               || LogProbSum: -52.299316184251495\n",
      "with               || LogProbSum: -52.692561181024885\n",
      "\n",
      "Best replacement: #\n"
     ]
    }
   ],
   "source": [
    "# malaria with prevention diagnosis and treatment ------ by involving non-health\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"malaria with prevention diagnosis and treatment\",\n",
    "    sentence_after=\"by involving non-health\",\n",
    "    n=3,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=trigram_model,\n",
    "    ngram_vocab=trigram_vocab,\n",
    "    ngram_token_count=trigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "care               || LogProbSum: -51.75622969826728\n",
      "related               || LogProbSum: -54.012294775626444\n",
      "sector               || LogProbSum: -54.176597826917714\n",
      "and               || LogProbSum: -54.618430579196755\n",
      "workers               || LogProbSum: -54.963271065488485\n",
      "professionals               || LogProbSum: -55.5628921880376\n",
      "sectors               || LogProbSum: -55.63700016019133\n",
      "key               || LogProbSum: -55.89936442465882\n",
      "of               || LogProbSum: -56.00472494031665\n",
      "consequences               || LogProbSum: -56.1870464971106\n",
      "\n",
      "Best replacement: care\n"
     ]
    }
   ],
   "source": [
    "# by involving non-health ----- stakeholders from provincial to community level\n",
    "_ = fill_blank(\n",
    "    sentence_before=\"by involving non health\",\n",
    "    sentence_after=\"stakeholders from provincial to community level\",\n",
    "    n=3,\n",
    "    smoothing_factor=1,\n",
    "    ngram_model=trigram_model,\n",
    "    ngram_vocab=trigram_vocab,\n",
    "    ngram_token_count=trigram_token_count,\n",
    "    keep_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Perplexity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_perplexity_score(sentence, n, ngram_model, ngram_token_count, smoothing_factor=1):\n",
    "    # convert the sentence to tuple form\n",
    "    sentence_tuple = tuple(word_tokenize(text_cleaner_2(sentence)))\n",
    "\n",
    "    # add sentence breaker before and after the sentence\n",
    "    sentence_tuple = ('#',) + sentence_tuple + ('#',)\n",
    "\n",
    "    # default smoothing prob for non-existent ngram tuples\n",
    "    # default_prob = smoothing_factor/ngram_token_count\n",
    "    default_logprob = np.log(smoothing_factor) - np.log(ngram_token_count)\n",
    "\n",
    "    # store the logprob some of ngrams of the sentence\n",
    "    sentence_logprob_sum = 0\n",
    "\n",
    "    for i in range(len(sentence_tuple)- n+1):\n",
    "        curr_logprob = default_logprob\n",
    "        curr_tuple = sentence_tuple[i:i+n]\n",
    "        if curr_tuple in ngram_model:\n",
    "            curr_logprob = np.log(ngram_model[curr_tuple])\n",
    "        sentence_logprob_sum += curr_logprob\n",
    "    \n",
    "    log_perplexity = (-1/(len(sentence_tuple)-2))*sentence_logprob_sum\n",
    "    perplexity = np.exp(log_perplexity)\n",
    "\n",
    "    return perplexity, log_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\n",
    "    \"it appears that the overall code stroke volume has decreased since the covid- pandemic\",\n",
    "    \"half a century ago hypertension was not treatable\",\n",
    "    \"sarahs tv is broadcasting an advert for private healthcare\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Scores:\n",
      "\n",
      "\n",
      "Sentence: it appears that the overall code stroke volume has decreased since the covid- pandemic\n",
      "Perplexity: 251869.6846089567\n",
      "Log Perplexity: 12.436667108170905\n",
      "--------------------------------------------------\n",
      "Sentence: half a century ago hypertension was not treatable\n",
      "Perplexity: 5833621.470202273\n",
      "Log Perplexity: 15.579148543897563\n",
      "--------------------------------------------------\n",
      "Sentence: sarahs tv is broadcasting an advert for private healthcare\n",
      "Perplexity: 103588138.32632214\n",
      "Log Perplexity: 18.455933386314943\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram Scores:\\n\\n\")\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    bigram_perplexity, bigram_log_perplexity = calculate_perplexity_score(\n",
    "                                                sentence=sentence,\n",
    "                                                n=2,\n",
    "                                                ngram_model=bigram_model,\n",
    "                                                ngram_token_count=bigram_token_count,\n",
    "                                                smoothing_factor=1\n",
    "                                                )\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Perplexity:\", bigram_perplexity)\n",
    "    print(\"Log Perplexity:\", bigram_log_perplexity)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Scores:\n",
      "\n",
      "\n",
      "Sentence: it appears that the overall code stroke volume has decreased since the covid- pandemic\n",
      "Perplexity: 4243647.472057198\n",
      "Log Perplexity: 15.26093371024641\n",
      "--------------------------------------------------\n",
      "Sentence: half a century ago hypertension was not treatable\n",
      "Perplexity: 28429719.457511067\n",
      "Log Perplexity: 17.16294561574561\n",
      "--------------------------------------------------\n",
      "Sentence: sarahs tv is broadcasting an advert for private healthcare\n",
      "Perplexity: 173296818.0463108\n",
      "Log Perplexity: 18.970516393561187\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Trigram Scores:\\n\\n\")\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    trigram_perplexity, trigram_log_perplexity = calculate_perplexity_score(\n",
    "                                sentence=sentence,\n",
    "                                n=3,\n",
    "                                ngram_model=trigram_model,\n",
    "                                ngram_token_count=trigram_token_count,\n",
    "                                smoothing_factor=1\n",
    "                                )\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Perplexity:\", trigram_perplexity)\n",
    "    print(\"Log Perplexity:\", trigram_log_perplexity)\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2395b043a5373adde0b52d24776553eeb7a0bda4cf211af66f470cac9504b9e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
